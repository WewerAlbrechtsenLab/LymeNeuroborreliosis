{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSF analysis\n",
    "\n",
    "This notebook replicates the CSF results reported in the publication, this includes source data and figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib.pyplot import gcf\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib_venn import venn3, venn2\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, iplot\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "import numpy as np \n",
    "import alphastats\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "init_notebook_mode()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- # Corrected data development -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# format data for AlphaPeptStats\n",
    "df_dev = pd.read_csv(\"../submission/aps_data_corrected_csf_development.tsv\",sep = '\\t')\n",
    "cols = df_dev.columns.tolist()\n",
    "df_dev.rename(columns={'Protein.Group':'Genes','Genes':'Protein.Group'}, inplace=True)\n",
    "df_dev[cols].to_csv(\"../submission/aps_data_corrected_csf_development_nameswap.tsv\", index = False, sep='\\t')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# also format validation\n",
    "df_val = pd.read_csv(\"../submission/aps_data_corrected_csf_validation.tsv\",sep = '\\t')\n",
    "cols = df_val.columns.tolist()\n",
    "df_val.rename(columns={'Protein.Group':'Genes','Genes':'Protein.Group'}, inplace=True)\n",
    "df_val[cols].to_csv(\"../submission/aps_data_corrected_csf_validation_nameswap.tsv\", index = False, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = alphastats.DIANNLoader(file = \"../submission/aps_data_corrected_csf_development_nameswap.tsv\")\n",
    "dataset = alphastats.DataSet(\n",
    "    loader = loader, \n",
    "    metadata_path=\"../submission/aps_meta_csf.xlsx\", \n",
    "    sample_column=\"sample_id\"\n",
    ")\n",
    "dataset.preprocess(\n",
    "    log2_transform=True,\n",
    "    remove_contaminations=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "volcano_plot = dataset.plot_volcano(\n",
    "    column=\"diagnosis\", # column in metadata\n",
    "    group1=\"LNB\",\n",
    "    group2=\"VM\",\n",
    "    labels=True, # add label to significantly enriched proteins\n",
    "    alpha=0.05 # cutoff for pvalue\n",
    ")\n",
    "\n",
    "pio.write_image(volcano_plot, '../submission/f1a.svg', format='svg')\n",
    "\n",
    "volcano_plot.show(renderer = \"png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "volcano_plot = dataset.plot_volcano(\n",
    "    column=\"diagnosis\", # column in metadata\n",
    "    group1=\"LNB\",\n",
    "    group2=\"control\",\n",
    "    labels=True, # add label to significantly enriched proteins\n",
    "    alpha=0.05 # cutoff for pvalue\n",
    ")\n",
    "\n",
    "pio.write_image(volcano_plot, '../submission/f1b.svg', format='svg')\n",
    "\n",
    "volcano_plot.show(renderer = \"png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csf_proteins = df_dev[['Genes']].drop_duplicates()\n",
    "print(csf_proteins.shape)\n",
    "csf_proteins['Protein.Group'] = csf_proteins['Genes'].str.split(';')\n",
    "csf_proteins = csf_proteins.explode('Protein.Group').drop_duplicates()\n",
    "print(csf_proteins.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Differential expression analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table to annotate proteins to GO\n",
    "annotation_table = pd.read_csv('../data/annotation_table.tsv', sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Differential expression analysis\n",
    "diffs = []\n",
    "dfs = []\n",
    "for groups in [['LNB','VM'],['LNB','control']]:\n",
    "    print(groups)\n",
    "    diff_df= dataset.diff_expression_analysis(groups[0],groups[1],column='diagnosis', fdr=0.05)\n",
    "    diff_df = pd.merge(df_dev[['Genes','Protein.Group']].drop_duplicates(),diff_df, on = 'Protein.Group')\n",
    "    diff_df.rename(columns={'Protein.Group':'Genes', 'Genes':'Protein.Group'}, inplace=True)\n",
    "    diff_df = diff_df[(diff_df.pval<0.05)]\n",
    "    diff_df = pd.merge(diff_df, annotation_test[['Entry','GO','Gene Names']], left_on = 'Protein.Group', right_on = 'Entry', how = 'left')\n",
    "    diff_df['group1']=groups[0]\n",
    "    diff_df['group2']=groups[1]\n",
    "    print(diff_df.shape)\n",
    "    print('UP: {}'.format(diff_df[(diff_df.log2fc>1)]['Gene Names'].tolist()))\n",
    "    print('DOWN: {}'.format(diff_df[(diff_df.log2fc<-1)]['Gene Names'].tolist()))\n",
    "    diff_df = diff_df[(diff_df.log2fc.abs()>1)]\n",
    "    dfs.append(diff_df)\n",
    "    print(diff_df.shape)\n",
    "    diff_df = diff_df['Protein.Group'].tolist()\n",
    "    print(len(diff_df))\n",
    "    diffs.append(diff_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([dfs[0], dfs[1]])[['group1', 'group2','Protein.Group', 'Genes', 'pval', 'log2fc', 'GO']].to_excel('../submission/st1.xlsx', index = None)\n",
    "print(dfs[0]['Protein.Group'].unique().shape)\n",
    "print(dfs[1]['Protein.Group'].unique().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 6))\n",
    "\n",
    "# Make the diagram\n",
    "venn2([set(diffs[0]), set(diffs[1])], set_labels=('LNB_VM','LNB_control'))\n",
    "\n",
    "#plt.savefig(\"../output/venn_diagram_csf_dev.svg\", format=\"svg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_lst = list(set(diffs[0]+diffs[1]))\n",
    "pd.DataFrame(combined_lst, columns = ['csf_identifiers']).to_csv('../submission/combined_lst.csv', index = None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info = pd.read_excel('../submission/aps_meta_csf.xlsx')\n",
    "aps_data = pd.read_table('../submission/aps_data_corrected_csf_development.tsv', sep ='\\t').drop(['Protein.Names','Genes','First.Protein.Description'], axis = 1)\n",
    "aps_data = aps_data.set_index(['Protein.Group']).T.reset_index()\n",
    "aps_data.rename(columns={'index':'sample_id'}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diagnosis_order = [\n",
    "    \"VM\",\n",
    "    \"LNB\",\n",
    "    \"control\"\n",
    "]\n",
    "# Convert diagnosis to categorical with your order\n",
    "info[\"diagnosis\"] = pd.Categorical(\n",
    "    info[\"diagnosis\"],\n",
    "    categories=diagnosis_order,\n",
    "    ordered=True\n",
    ")\n",
    "\n",
    "heatmap_data = np.log2(pd.merge(info.drop('group', axis =1), aps_data, on = 'sample_id')\n",
    "                       .sort_values(['diagnosis','study_cohort'])\n",
    "                       .set_index(['sample_id','diagnosis','study_cohort'])\n",
    "                       .T)\n",
    "print(heatmap_data.shape)\n",
    "heatmap_data = heatmap_data.loc[heatmap_data.index.get_level_values(0).isin(combined_lst)].T\n",
    "print(heatmap_data.shape)\n",
    "heatmap_data = (heatmap_data - heatmap_data.mean()) / heatmap_data.std()\n",
    "heatmap_data = heatmap_data.T\n",
    "\n",
    "# merge annotation with heatmap_data:\n",
    "annotation_table = pd.merge(pd.DataFrame(heatmap_data.index.tolist(), columns = ['Entry']), annotation_table[['Entry','GO','Gene Names']], on = 'Entry', how = 'left')\n",
    "annotation_table['Gene Names']=annotation_table['Gene Names'].str.split(' ').str[0]\n",
    "\n",
    "\n",
    "# 2. Add 'annotation' and 'Gene Names' from annotation_table to the index\n",
    "annotations = annotation_table['GO'].tolist()  # Replace 'GO' with the correct column name if needed\n",
    "gene_names = annotation_table['Gene Names'].tolist()  # Ensure 'Gene Names' is the correct column\n",
    "\n",
    "# Creating a MultiIndex\n",
    "heatmap_data.index = pd.MultiIndex.from_tuples(\n",
    "    list(zip(gene_names, heatmap_data.index)),\n",
    "    names=['Gene Names', 'Entry']\n",
    ")\n",
    "\n",
    "# grouping colors\n",
    "group_labels = heatmap_data.columns.get_level_values(\"diagnosis\").astype(str)\n",
    "group_pal = sns.color_palette('Set2',group_labels.unique().size)\n",
    "group_lut = dict(zip(map(str, group_labels.unique()), group_pal))\n",
    "# create group index\n",
    "group_colors = pd.Series(group_labels, index=heatmap_data.columns).map(group_lut)\n",
    "\n",
    "g1 = sns.clustermap(heatmap_data, z_score=False, col_cluster=False,\n",
    "                   figsize=(35,12), col_colors = group_colors, #row_colors = annotation_colors, \n",
    "                   linewidth=0.5, cmap = \"coolwarm\", center = 0,\n",
    "                   dendrogram_ratio=0.15, metric = 'euclidean', colors_ratio=0.018)\n",
    "\n",
    "# add legends\n",
    "for label in group_labels.unique():\n",
    "    g1.ax_col_dendrogram.bar(0, 0, color=group_lut[label], label=label, linewidth=0);\n",
    "l1 = g1.ax_col_dendrogram.legend(title='Group', loc=\"center\", ncol=7, bbox_to_anchor=(0.5, 1), bbox_transform=gcf().transFigure)\n",
    "g1.fig.subplots_adjust(right=0.7)\n",
    "\n",
    "g1.figure.savefig(\"../submission/f1c.svg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_linkage_dev = g1.dendrogram_row.linkage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sankey plot with GO groups of the heatmap proteins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap_order = heatmap_data.iloc[g1.dendrogram_row.reordered_ind].index.get_level_values(\"Entry\").tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sankey_df = pd.merge(pd.DataFrame(heatmap_data.index.get_level_values(\"Entry\").tolist(), columns = ['Entry']), annotation_table[['Entry','GO','Gene Names']].drop_duplicates(), on = 'Entry', how = 'left')\n",
    "sankey_df.fillna('nan', inplace = True)\n",
    "# sort the df according to heatmap\n",
    "sankey_df['Entry'] = pd.Categorical(sankey_df['Entry'], categories=heatmap_order, ordered=True)\n",
    "sankey_df = sankey_df.sort_values(['Entry'])\n",
    "# format the df\n",
    "sankey_df.Entry = sankey_df.Entry.astype(str)\n",
    "sankey_df['Gene Names'] = sankey_df['Gene Names'].str.split(' ', expand = True)[0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import random\n",
    "import plotly.io as pio\n",
    "\n",
    "# Data for Sankey plot\n",
    "\n",
    "# Get unique values of Genes and annotations\n",
    "all_Genes = list(sankey_df['Gene Names'].unique())\n",
    "#all_Genes.reverse()\n",
    "all_annotations = list(sankey_df['GO'].unique())\n",
    "\n",
    "\n",
    "# Create indices for sankey source (Gene Namess) and targets (annotations)\n",
    "sankey_df['Gene Names_index'] = sankey_df['Gene Names'].apply(lambda x: all_Genes.index(x))\n",
    "sankey_df = sankey_df.sort_values(by='Gene Names_index').reset_index(drop=True)  # Ensure sankey_df is sorted by the order of 'all_Genes'\n",
    "sankey_df['annotation_index'] = sankey_df['GO'].apply(lambda x: len(all_Genes) + all_annotations.index(x))\n",
    "\n",
    "# Generate a color for each annotation\n",
    "colors = ['#'+''.join([random.choice('0123456789ABCDEF') for j in range(6)]) for i in range(len(all_annotations))]\n",
    "\n",
    "# Map annotation index to colors\n",
    "target_colors = {len(all_Genes) + i: colors[i] for i in range(len(all_annotations))}\n",
    "\n",
    "# Create a list of colors for the links based on target node\n",
    "link_colors = [target_colors[target] for target in sankey_df['annotation_index']]\n",
    "\n",
    "# Define node positions for source nodes (x = 0.1 for all sources, y spaced evenly)\n",
    "source_y_positions = [i / len(all_Genes) for i in range(len(all_Genes))]  # Evenly spaced source y positions\n",
    "source_x_positions = [0.1] * len(all_Genes)  # Fixed x position for sources\n",
    "\n",
    "# Only set positions for sources, leave target positions to be automatically placed\n",
    "node_x_positions = source_x_positions + [None] * len(all_annotations)  # No x positions for targets\n",
    "node_y_positions = source_y_positions + [None] * len(all_annotations)  # No y positions for targets\n",
    "\n",
    "# Create Sankey diagram\n",
    "fig = go.Figure(go.Sankey(\n",
    "    node = dict(\n",
    "        pad = 15,\n",
    "        thickness = 20,\n",
    "        line = dict(color = \"black\", width = 0.5),\n",
    "        label = all_Genes + all_annotations,\n",
    "        x = node_x_positions,\n",
    "        y = node_y_positions\n",
    "    ),\n",
    "    link = dict(\n",
    "        source = sankey_df['Gene Names_index'],  # starting points (Gene Namess)\n",
    "        target = sankey_df['annotation_index'],  # end points (annotations)\n",
    "        value = [1] * len(sankey_df),  # each link has equal value (1 for each protein-GO link)\n",
    "        color = link_colors  # set the color of links\n",
    "    )\n",
    "))\n",
    "\n",
    "# Update layout and display the figure\n",
    "fig.update_layout(title_text=\"Sankey Diagram of Protein Gene Names and Annotations\", \n",
    "                  font_size=10,\n",
    "                  height=800,\n",
    "                  width=600)\n",
    "\n",
    "# Define the path where you want to save the PDF\n",
    "output_path = '../submission/f1c_sankey.svg'\n",
    "\n",
    "# Save the figure \n",
    "pio.write_image(fig, output_path, format='svg')\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSF Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = alphastats.DIANNLoader(file = \"../submission/aps_data_corrected_csf_validation_nameswap.tsv\")\n",
    "dataset = alphastats.DataSet(\n",
    "    loader = loader, \n",
    "    metadata_path=\"../submission/aps_meta_csf.xlsx\", \n",
    "    sample_column=\"sample_id\"\n",
    ")\n",
    "dataset.preprocess(\n",
    "    log2_transform=True,\n",
    "    remove_contaminations=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "volcano_plot = dataset.plot_volcano(\n",
    "    column=\"diagnosis\", # column in metadata\n",
    "    group1=\"LNB\",\n",
    "    group2=\"control\",\n",
    "    labels=True, # add label to significantly enriched proteins\n",
    "    alpha=0.05 # cutoff for pvalue\n",
    ")\n",
    "\n",
    "pio.write_image(volcano_plot, '../submission/s2.svg', format='svg')\n",
    "\n",
    "volcano_plot.show(renderer = \"png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "volcano_plot = dataset.plot_volcano(\n",
    "    column=\"diagnosis\", # column in metadata\n",
    "    group1=\"LNB\",\n",
    "    group2=\"VM\",\n",
    "    labels=True, # add label to significantly enriched proteins\n",
    "    alpha=0.05 # cutoff for pvalue\n",
    ")\n",
    "\n",
    "pio.write_image(volcano_plot, '../submission/s3.svg', format='svg')\n",
    "\n",
    "volcano_plot.show(renderer = \"png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diffs = []\n",
    "for groups in [['LNB','VM'],['LNB','control']]:\n",
    "    print(groups)\n",
    "    diff_df= dataset.diff_expression_analysis(groups[0],groups[1],column='diagnosis', fdr=0.05)\n",
    "    diff_df = pd.merge(df_val[['Genes','Protein.Group']].drop_duplicates(),diff_df, on = 'Protein.Group')\n",
    "    diff_df.rename(columns={'Protein.Group':'Genes', 'Genes':'Protein.Group'}, inplace=True)\n",
    "    diff_df = diff_df[(diff_df.pval<0.05)]\n",
    "    #diff_df = pd.merge(diff_df, annotation_table[['Entry','GO','Gene Names']], left_on = 'Protein.Group', right_on = 'Entry', how = 'left')\n",
    "\n",
    "    print(diff_df.shape)\n",
    "    print('UP: {}'.format(diff_df[(diff_df.log2fc>1)]['Genes'].tolist()))\n",
    "    print('DOWN: {}'.format(diff_df[(diff_df.log2fc<-1)]['Genes'].tolist()))\n",
    "    diff_df = diff_df[(diff_df.log2fc.abs()>1)]['Protein.Group'].tolist()\n",
    "    print(len(diff_df))\n",
    "    diffs.append(diff_df)\n",
    "combined_lst_val = list(set(diffs[0]+diffs[1]))\n",
    "print(len(combined_lst_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison between validation and development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison to Development Cohort\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "\n",
    "# Make the diagram\n",
    "venn2([set(combined_lst), set(combined_lst_val)], set_labels=('dev','val'))\n",
    "\n",
    "#plt.savefig(\"../output/venn_diagram_csf_val_dev.svg\", format=\"svg\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation Heatmap\n",
    "info = pd.read_excel('../submission/aps_meta_csf.xlsx')\n",
    "aps_data = pd.read_table('../submission/aps_data_corrected_csf_validation.tsv', sep ='\\t').drop(['Protein.Names','Genes','First.Protein.Description'], axis = 1)\n",
    "aps_data = aps_data.set_index(['Protein.Group']).T.reset_index()\n",
    "aps_data.rename(columns={'index':'sample_id'}, inplace=True)\n",
    "\n",
    "diagnosis_order = [\n",
    "    \"VM\",\n",
    "    \"LNB\",\n",
    "    \"control\"\n",
    "]\n",
    "# Convert diagnosis to categorical with your order\n",
    "info[\"diagnosis\"] = pd.Categorical(\n",
    "    info[\"diagnosis\"],\n",
    "    categories=diagnosis_order,\n",
    "    ordered=True\n",
    ")\n",
    "\n",
    "heatmap_data = np.log2(pd.merge(info.drop('group', axis =1), aps_data, on = 'sample_id')\n",
    "                       .set_index(['sample_id','diagnosis','study_cohort'])\n",
    "                       .T)\n",
    "heatmap_data = heatmap_data.loc[heatmap_data.index.get_level_values(0).isin(combined_lst)].T\n",
    "heatmap_data = (heatmap_data - heatmap_data.mean()) / heatmap_data.std()\n",
    "heatmap_data = heatmap_data.T\n",
    "\n",
    "# Compute the mean\n",
    "mean_df = heatmap_data.T\n",
    "heatmap_data = mean_df.groupby(mean_df.index.get_level_values('diagnosis')).mean().reset_index()\n",
    "heatmap_data = heatmap_data.sort_values(['diagnosis']).set_index(['diagnosis']).T\n",
    "\n",
    "\n",
    "# merge annotation with heatmap_data:\n",
    "annotation_table = pd.merge(pd.DataFrame(heatmap_data.index.tolist(), columns = ['Entry']), annotation_table[['Entry','GO','Gene Names']].drop_duplicates(), on = 'Entry', how = 'left')\n",
    "annotation_table['Gene Names']=annotation_table['Gene Names'].str.split(' ').str[0]\n",
    "\n",
    "\n",
    "# 2. Add 'annotation' and 'Gene Names' from annotation_table to the index\n",
    "annotations = annotation_table['GO'].tolist()  # Replace 'GO' with the correct column name if needed\n",
    "gene_names = annotation_table['Gene Names'].tolist()  # Ensure 'Gene Names' is the correct column\n",
    "\n",
    "# Creating a MultiIndex\n",
    "heatmap_data.index = pd.MultiIndex.from_tuples(\n",
    "    list(zip(gene_names, heatmap_data.index)),\n",
    "    names=['Gene Names', 'Entry']\n",
    ")\n",
    "\n",
    "# grouping colors\n",
    "group_labels = heatmap_data.columns.get_level_values(\"diagnosis\").astype(str)\n",
    "group_pal = sns.color_palette('Set2',group_labels.unique().size)\n",
    "group_lut = dict(zip(map(str, group_labels.unique()), group_pal))\n",
    "# create group index\n",
    "group_colors = pd.Series(group_labels, index=heatmap_data.columns).map(group_lut)\n",
    "\n",
    "g = sns.clustermap(heatmap_data, z_score=False, col_cluster=False,\n",
    "                   row_linkage=row_linkage_dev,  # Use row_linkage to cluster rows\n",
    "                   figsize=(35,12), col_colors=group_colors, \n",
    "                   linewidth=0.5, cmap=\"inferno\", center=0,\n",
    "                   dendrogram_ratio=0.15, metric='euclidean', colors_ratio=0.018)\n",
    "\n",
    "# add legends\n",
    "for label in group_labels.unique():\n",
    "    g.ax_col_dendrogram.bar(0, 0, color=group_lut[label], label=label, linewidth=0);\n",
    "l1 = g.ax_col_dendrogram.legend(title='Group', loc=\"center\", ncol=7, bbox_to_anchor=(0.5, 1), bbox_transform=gcf().transFigure)\n",
    "\n",
    "g1.figure.savefig(\"../submission/f1c_val.svg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- # ML analysis -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diagnostic Machine Learning Classification: LNB vs. control"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strategy:\n",
    "1) subset ML input to LNB and Control\n",
    "2) Run nested cross-validation to: 1) select optimal hyperparameters and features 2) select optimal algorithm\n",
    "3) After finishing model development, test performance on the validation cohort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ML data\n",
    "ml_data = pd.read_csv(\"../submission/aps_data_corrected_csf_combined.tsv\", sep = '\\t')\n",
    "ml_data = ml_data.set_index(['Protein.Group']).drop(['Protein.Names','Genes','First.Protein.Description'], axis = 1).T.reset_index()\n",
    "ml_data.rename(columns={'index':'sample_id'}, inplace=True)\n",
    "\n",
    "ml_data = np.log2(pd.merge(info,#.replace({'study_cohort':{'csf_development':'csf_validation', 'csf_validation':'csf_development'}}), \n",
    "                           ml_data, on = 'sample_id')\n",
    "                       .sort_values(['diagnosis','study_cohort'])\n",
    "                       .set_index(['sample_id','diagnosis','study_cohort']).astype(float))\n",
    "ml_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset data to the significant findings\n",
    "ml_data = ml_data[combined_lst]\n",
    "ml_data.index.set_names(['_sample','_diagnosis','_study'], inplace = True)\n",
    "print(ml_data.index.get_level_values('_study').value_counts())\n",
    "# change column names to gene names\n",
    "gene_mapping_df = uniprot_mapping[uniprot_mapping.Entry.isin(combined_lst)][['Entry','Gene Names']].drop_duplicates()\n",
    "gene_mapping_df['Gene Names']=gene_mapping_df['Gene Names'].str.split(' ').str[0].str.split(';').str[0]\n",
    "gene_mapping_df.set_index('Entry', inplace = True)\n",
    "\n",
    "ml_data.columns = gene_mapping_df.loc[combined_lst]['Gene Names'].tolist()\n",
    "ml_data.to_csv('../submission/csf_ml_data.tsv', sep = '\\t')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare sklearn models and grids\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, matthews_corrcoef, accuracy_score, f1_score, precision_score, recall_score, roc_auc_score, classification_report, roc_curve\n",
    "from scipy import interp\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix, matthews_corrcoef, accuracy_score, f1_score, precision_score, recall_score, roc_auc_score, classification_report, roc_curve\n",
    "\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, StratifiedKFold, cross_val_score\n",
    "from sklearn.feature_selection import RFECV\n",
    "from scipy.stats import zscore\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier, ExtraTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn import ensemble\n",
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "from scipy.stats import uniform, truncnorm, randint\n",
    "\n",
    "import time\n",
    "\n",
    "n_features = len(combined_lst)\n",
    "n_samples = int(ml_data.loc[ml_data.index.get_level_values('_study') == 'csf_development'].shape[0]*0.8)\n",
    "\n",
    "\n",
    "# hyperparameter grids\n",
    "lr_grid = {\"C\": [0.1, 0.5, 1],#np.logspace(0, 4, num=10),\n",
    "           \"penalty\": ['l1', 'l2','elasticnet'],\n",
    "           \"solver\": ['liblinear', 'saga'],\n",
    "           \"max_iter\":[5000]}\n",
    "lda_grid = {\"solver\": ['svd', 'lsqr', 'eigen'],\n",
    "            \"shrinkage\":[None, 'auto']}\n",
    "knn_grid = {\"n_neighbors\": randint(1,11),\n",
    "            \"weights\":['uniform','distance'], \n",
    "            \"algorithm\":['auto','ball_tree','kd_tree','brute'], \n",
    "            \"leaf_size\":randint(10,50), \n",
    "            \"p\":[1,2],\n",
    "            \"metric\":['euclidean','minkowski','mahalanobis','seuclidean']}\n",
    "dt_grid = {\"criterion\": [\"gini\", \"entropy\"],\n",
    "            \"splitter\": ['best','random'],\n",
    "            #\"max_features\": randint(1, n_features),\n",
    "            \"min_samples_leaf\": randint(1, 11),\n",
    "            \"class_weight\":[None, 'balanced']}\n",
    "nb_grid = {}\n",
    "svc_grid = {\"kernel\": ['linear'], #'poly', 'rbf', 'sigmoid'], #removed because it does not work with importance getter\n",
    "            \"gamma\":['scale','auto'],\n",
    "            \"class_weight\" : ['balanced',None],\n",
    "            \"probability\" : [True]}\n",
    "rf_grid = {\"max_depth\": [3, None],\n",
    "            \"min_samples_split\": randint(1, 11),\n",
    "            \"min_samples_leaf\": randint(1, 11),\n",
    "            \"bootstrap\": [True, False],\n",
    "            \"criterion\": [\"gini\", \"entropy\"],\n",
    "            \"n_estimators\": randint(1,200),\n",
    "            \"max_features\": truncnorm(a=0, b=1, loc=0.25, scale=0.1)}\n",
    "nnet_grid = {\"hidden_layer_sizes\":randint(1,n_features),\n",
    "            \"activation\": ['identity','logistic','tanh','relu'],\n",
    "            \"solver\":['lbfgs','sgd','adam'],\n",
    "            \"batch_size\":[int(n_samples/10),int(n_samples/5), int(n_samples/2)],\n",
    "            \"learning_rate_init\":np.logspace(np.log10(0.00001), np.log10(0.99999), num=10),\n",
    "            \"max_iter\":[5000],\n",
    "            \"early_stopping\":[True],\n",
    "            \"validation_fraction\":[0.25]}\n",
    "perceptron_grid = {\"penalty\": ['l1', 'l2','elasticnet'],\n",
    "                   \"max_iter\":[5000],\n",
    "                   \"class_weight\" : ['balanced',None]}\n",
    "                   \n",
    "gp_grid = {}\n",
    "ab_grid = {}\n",
    "qda_grid = {}\n",
    "eet_grid = {}\n",
    "\n",
    "# models\n",
    "models = []\n",
    "models.append(('LR', lr_grid, LogisticRegression(random_state=7)))\n",
    "models.append(('LDA', lda_grid, LinearDiscriminantAnalysis()))\n",
    "models.append(('KNN', knn_grid, KNeighborsClassifier()))\n",
    "models.append(('DT', dt_grid, DecisionTreeClassifier(random_state=7)))\n",
    "models.append(('Naive Bayes', nb_grid, GaussianNB()))\n",
    "models.append(('SVC', svc_grid, SVC(random_state=7)))\n",
    "models.append(('RF', rf_grid, RandomForestClassifier(random_state=7)))\n",
    "models.append(('NNet', nnet_grid, MLPClassifier(random_state=7)))\n",
    "models.append(('GP', gp_grid, GaussianProcessClassifier(random_state=7)))\n",
    "models.append(('Adaboost', ab_grid, AdaBoostClassifier(random_state=7)))\n",
    "models.append(('QDA', qda_grid, QuadraticDiscriminantAnalysis()))\n",
    "models.append(('ensembleextratree', eet_grid, ensemble.ExtraTreesClassifier(random_state=7)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare ML data by zscoring and formatting\n",
    "prepared_data = ml_data.copy()\n",
    "prepared_data=zscore(prepared_data, axis = 1)\n",
    "train_data = prepared_data.loc[(prepared_data.index.get_level_values('_study')=='csf_development')&(prepared_data.index.get_level_values('_diagnosis').isin(['LNB','VM']))]\n",
    "\n",
    "no_cv = 5\n",
    "X = train_data.values # all the protein columns\n",
    "sample_lst = train_data.index.get_level_values('_sample').values\n",
    "y = pd.Categorical(train_data.index.get_level_values('_diagnosis')).codes # LNB 0 and VM 1\n",
    "\n",
    "X_train = X.copy()\n",
    "y_train = y.copy()\n",
    "sum(y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data = prepared_data.loc[(prepared_data.index.get_level_values('_study')=='csf_validation')&(prepared_data.index.get_level_values('_diagnosis').isin(['LNB','VM']))]\n",
    "X_val = val_data.values\n",
    "y_val = pd.Categorical(val_data.index.get_level_values('_diagnosis')).codes\n",
    "sum(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run nested cross-validation on development data\n",
    "\n",
    "from itertools import compress\n",
    "protein_cols = gene_mapping_df.loc[combined_lst]['Gene Names'].tolist()\n",
    "# ML alg. selection\n",
    "optimal_models = []\n",
    "optimal_no_features = []\n",
    "optimal_features = []\n",
    "start_time = time.time()\n",
    "\n",
    "dfs = []\n",
    "metrics = []\n",
    "to_plot = []   \n",
    "fitted_models = []\n",
    "    \n",
    "for name, grid, estimator in models:\n",
    "    print(name)\n",
    "    # grid search\n",
    "    selector = RandomizedSearchCV(estimator=estimator, param_distributions=grid, cv=StratifiedKFold(no_cv), scoring = 'f1_weighted', n_jobs = -1)\n",
    "    clf = selector.fit(X_train,y_train)\n",
    "    optimal_model = clf.best_estimator_\n",
    "    optimal_models.append(optimal_model)\n",
    "    # feature reduction\n",
    "    if name in ['Naive Bayes','NNet', 'GP', 'QDA', 'KNN']: # skip rfecv for models not supported by importance\n",
    "        optimal_no_features.append(len(protein_cols))\n",
    "        optimal_features.append(protein_cols)\n",
    "        print('skipping feature importance')\n",
    "        X_reduced = X_train\n",
    "        X_val_reduced = X_val\n",
    "    else:\n",
    "        selector = RFECV(optimal_model, cv=5, scoring='f1_weighted')\n",
    "        selector = selector.fit(X_train, y_train)\n",
    "        optimal_no_features.append(sum(selector.support_))\n",
    "        optimal_features.append(list(compress(protein_cols, selector.support_)))\n",
    "        print('optimal no features = {}'.format(sum(selector.support_)))\n",
    "        X_reduced = X_train[:,selector.support_]\n",
    "        X_val_reduced = X_val[:, selector.support_]\n",
    "    \n",
    "    # compute cv results\n",
    "    print('computing predictions for: {}'.format(name))\n",
    "    classifier = optimal_model\n",
    "    mccs = []\n",
    "    aucs = []\n",
    "    fits = []\n",
    "    mcc_val = []\n",
    "    auc_val = []\n",
    "                            \n",
    "    # calculate performance during training\n",
    "    cv = StratifiedKFold(n_splits=no_cv)\n",
    "    for i, (train, test) in enumerate(cv.split(X_reduced, y_train)):\n",
    "        print('cv: {}'.format(i+1))\n",
    "        classifier.fit(X_reduced[train], y_train[train])\n",
    "        \n",
    "        fits.append(classifier)\n",
    "        # predict for \"test\" set\n",
    "        y_pred = classifier.predict(X_reduced[test])\n",
    "        y_prob = classifier.predict_proba(X_reduced[test])[:,1]\n",
    "        result = pd.DataFrame([y_train[test], y_pred, y_prob]).T\n",
    "        result.columns = ['observed','predicted','probability']\n",
    "        result['cv']=i+1\n",
    "        result['model']=name\n",
    "        result['sample']=sample_lst[test]\n",
    "        result['run']='test'\n",
    "        dfs.append(result)\n",
    "        mccs.append(round(matthews_corrcoef(result.observed, result.predicted),2))\n",
    "        aucs.append(round(roc_auc_score(result.observed,result.probability),2))\n",
    "        # predict for validation sets\n",
    "        y_pred = classifier.predict(X_val_reduced)\n",
    "        y_prob = classifier.predict_proba(X_val_reduced)[:,1]\n",
    "        result = pd.DataFrame([y_val, y_pred, y_prob]).T\n",
    "        result.columns = ['observed','predicted','probability']\n",
    "        result['cv']=i+1\n",
    "        result['model']=name\n",
    "        result['sample']=val_data.index.get_level_values('_sample').tolist()\n",
    "        result['run']='val'\n",
    "        dfs.append(result)  \n",
    "        mcc_val.append(round(matthews_corrcoef(result.observed, result.predicted),2))\n",
    "        auc_val.append(round(roc_auc_score(result.observed,result.probability),2))      \n",
    "        \n",
    "    fitted_models.append(fits)\n",
    "    #test\n",
    "    metrics_results = pd.DataFrame(mccs)\n",
    "    metrics_results.columns = ['MCC']\n",
    "    metrics_results['AUROC']=aucs\n",
    "    metrics_results['cv']=list(range(1,1+no_cv))\n",
    "    metrics_results['model']=name\n",
    "    metrics_results['run']='test'\n",
    "    metrics.append(metrics_results)\n",
    "\n",
    "    #val\n",
    "    metrics_results = pd.DataFrame(mccs)\n",
    "    metrics_results.columns = ['MCC']\n",
    "    metrics_results['AUROC']=aucs\n",
    "    metrics_results['cv']=list(range(1,1+no_cv))\n",
    "    metrics_results['model']=name\n",
    "    metrics_results['run']='val'\n",
    "    metrics.append(metrics_results)\n",
    "    \n",
    "    mcc_mean = np.mean(mccs)\n",
    "    mcc_std = np.std(mccs)\n",
    "    auroc_mean = np.mean(aucs)\n",
    "    auroc_std = np.std(aucs)\n",
    "    to_plot.append([name, mcc_mean, mcc_std, auroc_mean, auroc_std])\n",
    "    print('MCC: {}, AUROC: {}'.format(mcc_mean, auroc_mean))\n",
    "    \n",
    "    print('\\n{} model tested --- {} minutes ---\\n'.format(name,round(((time.time() - start_time)/60),3)))\n",
    "\n",
    "\n",
    "result_df = pd.concat(dfs)\n",
    "metrics_df = pd.concat(metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare to plot of the performance\n",
    "plot_df = pd.DataFrame(to_plot)\n",
    "plot_df.columns=['model','mcc_mean','mcc_std','auroc_mean','auroc_std']\n",
    "plot_df = round(plot_df,2)\n",
    "plot_df['no_features']=[len(x) for x in optimal_features]\n",
    "plot_df['features']=optimal_features\n",
    "plot_df = plot_df.sort_values('mcc_mean').reset_index(drop = True)\n",
    "plot_df['model_n_features']= ['{} (no. proteins = {})'.format(plot_df.loc[i,'model'],plot_df.loc[i,'no_features']) for i in plot_df.index.tolist()]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot performance across the ML models\n",
    "from helper_functions import butterfly_plot\n",
    "butterfly_plot(plot_df, var1 = 'mcc_mean', var2 = 'auroc_mean', var3 = 'no_features', error1 = 'mcc_std', error2 = 'auroc_std', name = 's4', savesvg=True, group = 'model_n_features')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the optimal number of proteins by recursive feature elimination\n",
    "selector = RFECV(optimal_models[0], cv=5, scoring='f1_weighted')\n",
    "selector = selector.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Optimal number of features: {selector.n_features_}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot RFECV\n",
    "model_name = 'LR'\n",
    "error_max = selector.cv_results_['mean_test_score']+selector.cv_results_['std_test_score']\n",
    "error_min = selector.cv_results_['mean_test_score']-selector.cv_results_['std_test_score']\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=np.concatenate([list(range(1,len(protein_cols)+1)),list(range(1,len(protein_cols)+1))[::-1]]),\n",
    "    y=np.concatenate([error_max, error_min[::-1]]),\n",
    "    fill='toself', opacity=0.5, name = '1 std.'))\n",
    "fig.add_trace(go.Scatter(x=list(range(1,len(protein_cols)+1)), y = selector.cv_results_['mean_test_score'],\n",
    "                         name='Mean test score',line = dict(color='firebrick', width=1)))\n",
    "fig.add_trace(go.Scatter(x = [selector.n_features_,selector.n_features_], name = 'Optimal number of features: {}'.format(selector.n_features_), \n",
    "                         y=[0,1], \n",
    "                         mode='lines', \n",
    "                         line=dict(color='green', width=2, dash='dash')))\n",
    "\n",
    "fig.update_layout(template='simple_white',\n",
    "    autosize=False,\n",
    "    width=800,\n",
    "    height=500,\n",
    "    title=\"RFECV for {} classifier\".format(model_name),\n",
    "    xaxis_title=\"Number of features selected\",\n",
    "    yaxis_title=\"F1-weighted\",\n",
    "    legend_title=\"\",\n",
    "    #legend=dict(y=1.1, orientation='h'),\n",
    "    legend_traceorder=\"reversed\",\n",
    "    xaxis_range=[0,len(protein_cols)],\n",
    "    yaxis_range = [0,max(error_max)])\n",
    "\n",
    "#fig.write_image('../submission/RFECV_{}.pdf'.format(model_name))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save top features\n",
    "topfeatures_identifiers = list(compress(protein_cols, selector.support_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = zscore(X_train[:, selector.support_], axis = 1)\n",
    "X_val = zscore(X_val[:, selector.support_], axis =1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the predictions for ROC plotting\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "# compute cv results\n",
    "print('computing predictions for: {}'.format(optimal_models[0]))\n",
    "classifier = optimal_models[0]\n",
    "name = 'LR'\n",
    "mccs = []\n",
    "aucs = []\n",
    "fits = []\n",
    "mcc_val = []\n",
    "auc_val = []\n",
    "dfs = []\n",
    "                        \n",
    "# calculate performance during training \n",
    "cv = StratifiedKFold(n_splits=no_cv)\n",
    "for i, (train, test) in enumerate(cv.split(X_train, y_train)):\n",
    "    print('cv: {}'.format(i+1))\n",
    "    classifier.fit(X_train[train], y_train[train])\n",
    "    #calibrated = CalibratedClassifierCV(classifier, method='sigmoid', cv=5)\n",
    "    #calibrated.fit(X_train[train], y_train[train])\n",
    "    #classifier = calibrated\n",
    "    fits.append(classifier)\n",
    "    # predict for \"test\" set\n",
    "    y_pred = classifier.predict(X_train[test])\n",
    "    y_prob = classifier.predict_proba(X_train[test])[:,1]\n",
    "    result = pd.DataFrame([y_train[test], y_pred, y_prob]).T\n",
    "    result.columns = ['observed','predicted','probability']\n",
    "    result['cv']=i+1\n",
    "    result['model']=name\n",
    "    result['sample']=sample_lst[test]\n",
    "    result['run']='test'\n",
    "    dfs.append(result)\n",
    "    mccs.append(round(matthews_corrcoef(result.observed, result.predicted),2))\n",
    "    aucs.append(round(roc_auc_score(result.observed,result.probability),2))\n",
    "    # predict for validation sets\n",
    "    y_pred = classifier.predict(X_val)\n",
    "    y_prob = classifier.predict_proba(X_val)[:,1]\n",
    "    result = pd.DataFrame([y_val, y_pred, y_prob]).T\n",
    "    result.columns = ['observed','predicted','probability']\n",
    "    result['cv']=i+1\n",
    "    result['model']=name\n",
    "    result['sample']=val_data.index.get_level_values('_sample').tolist()\n",
    "    result['run']='val'\n",
    "    dfs.append(result)  \n",
    "    mcc_val.append(round(matthews_corrcoef(result.observed, result.predicted),2))\n",
    "    auc_val.append(round(roc_auc_score(result.observed,result.probability),2))      \n",
    "    \n",
    "mcc_mean = np.mean(mccs)\n",
    "mcc_std = np.std(mccs)\n",
    "auroc_mean = np.mean(aucs)\n",
    "auroc_std = np.std(aucs)\n",
    "to_plot.append([name, mcc_mean, mcc_std, auroc_mean, auroc_std])\n",
    "print('Test: MCC: {}, AUROC: {}'.format(mcc_mean, auroc_mean))\n",
    "print('Validation: MCC: {}, AUROC: {}'.format(np.mean(mcc_val), np.mean(auc_val)))\n",
    "\n",
    "result = pd.concat(dfs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare roc_data\n",
    "import scipy.stats as st\n",
    "def roc_data(df, model, run):\n",
    "    model_df = df.loc[(df.model == model)&(df.run == run)]\n",
    "    fpr_mean = np.linspace(0, 1, 100)\n",
    "    interp_tprs = []\n",
    "    aucs = []\n",
    "    mccs = []\n",
    "    for cv in list(range(1,5+1)):\n",
    "        model_df_sub = model_df.loc[model_df.cv == cv]\n",
    "        fpr, tpr, thresholds = roc_curve(model_df_sub.observed, model_df_sub.probability, drop_intermediate=False)\n",
    "        interp_tpr = np.interp(fpr_mean, fpr, tpr)\n",
    "        interp_tpr[0] = 0.0\n",
    "        interp_tprs.append(interp_tpr)\n",
    "        aucs.append(roc_auc_score(model_df_sub.observed, model_df_sub.probability))\n",
    "        mccs.append(matthews_corrcoef(model_df_sub.observed, model_df_sub.predicted))\n",
    "        #print(roc_auc_score(model_df_sub.observed, model_df_sub.probability))\n",
    "    tpr_mean     = np.mean(interp_tprs, axis=0)\n",
    "    tpr_mean[-1] = 1.0\n",
    "    tpr_std      = np.round(np.std(interp_tprs, axis=0),4)\n",
    "    tpr_upper    = tpr_mean+tpr_std\n",
    "    tpr_lower    = tpr_mean-tpr_std\n",
    "    auc_mean = np.round(np.mean(aucs)*100,2)\n",
    "    auc_sd = np.round(np.std(aucs)*100,2)\n",
    "    ci_lower, ci_upper = np.round([100*x for x in st.t.interval(alpha=0.95, df=len(aucs)-1,loc=np.mean(aucs),scale=st.sem(aucs))],2)\n",
    "    \n",
    "    mcc_mean = np.round(np.mean(mccs),4)\n",
    "    mcc_sd = np.round(np.std(mccs),4)\n",
    "    \n",
    "    return(fpr_mean, tpr_mean, tpr_upper, tpr_lower, auc_mean, auc_sd, ci_upper, ci_lower, mcc_mean, mcc_sd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'LR'\n",
    "\n",
    "fig = go.Figure()\n",
    "# ROC for TEST\n",
    "test_data = roc_data(result, model = name, run = \"test\")\n",
    "# output_order = test_data[0], test_data[1], test_data[2], test_data[3], test_data[4], auc_sd, ci_upper, ci_lower, mcc_mean, mcc_std\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=np.concatenate([test_data[0],test_data[0][::-1]]),\n",
    "    y=np.concatenate([test_data[2], test_data[3][::-1]]),\n",
    "    fill='toself', opacity=0.5, name = 'std.'))\n",
    "print('TEST auc = {} (std. = {}, ci = {}-{}), mcc = {} (std. = {}))'.format(test_data[4], test_data[5], test_data[6], test_data[7], test_data[8], test_data[9]))\n",
    "fig.add_trace(go.Scatter(x = test_data[0], name = 'TEST auc = {} (std. = {}, ci = {}-{}), mcc = {} (std. = {}))'.format(test_data[4], test_data[5], test_data[6], test_data[7], test_data[8], test_data[9]), \n",
    "                         y=test_data[1], \n",
    "                         mode='lines', \n",
    "                         line=dict(color='grey', width=2, dash='dash')))\n",
    "test_data = roc_data(result, model = name, run = \"val\")\n",
    "print('VAL auc = {} (std. = {}, ci = {}-{}), mcc = {} (std. = {}))'.format(test_data[4], test_data[5], test_data[6], test_data[7], test_data[8], test_data[9]))\n",
    "fig.add_trace(go.Scatter(x = test_data[0], name = 'VAL auc = {} (std. = {}, ci = {}-{}), mcc = {} (std. = {}))'.format(test_data[4], test_data[5], test_data[6], test_data[7], test_data[8], test_data[9]), \n",
    "                         y=test_data[1], \n",
    "                         mode='lines', \n",
    "                         line=dict(color='green', width=2)))\n",
    "\n",
    "fig.update_layout(template='simple_white',\n",
    "    autosize=False,\n",
    "    width=1000,\n",
    "    height=500,\n",
    "    title=\"AUROC for LR classifier\",\n",
    "    xaxis_title=\"TPR\",\n",
    "    yaxis_title=\"FPR\",\n",
    "    legend_title=\"\",\n",
    "    legend_traceorder=\"reversed\",\n",
    "    xaxis_range=[0,1],\n",
    "    yaxis_range = [0,1])\n",
    "fig.write_image('../submission/f2a.svg', width = 950)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shaply feature importance\n",
    "import shap\n",
    "\n",
    "X_train_df = pd.DataFrame(X_train, columns = topfeatures_identifiers)\n",
    "X_test_df = pd.DataFrame(X_val, columns = topfeatures_identifiers)\n",
    "\n",
    "# Generate the Tree SHAP estimator of Shapley values that corresponds to the LDA we built\n",
    "explainer = shap.LinearExplainer(optimal_models[0], X_train_df)\n",
    "# Compute the estimated Shapley values for the test sample's observations\n",
    "shap_values = explainer.shap_values(X_train_df)\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "shap.summary_plot(shap_values, X_train_df,max_display=17,show=False)\n",
    "\n",
    "plt.savefig('../submission/f2b.svg', format = 'svg') #.png,.pdf will also support here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = result[(result.model==name)&(result.run=='val')].observed\n",
    "y_pred = result[(result.model==name)&(result.run=='val')].predicted\n",
    "confusion_m = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(3,3))\n",
    "s = sns.heatmap(confusion_m, annot = True, cbar = False, fmt = \"d\", linewidths = .5, cmap = \"Blues\",\n",
    "           xticklabels=['LNB','control'], yticklabels=['LNB','control'])\n",
    "s.set(xlabel=\"Predicted class\", ylabel='Actual class')\n",
    "s.set(title = \"Confusion Matrix\")\n",
    "fig.tight_layout()\n",
    "#fig.savefig('/Users/tzx804/projects/collaborations/nicolai/misc/sepsis/output/test_confusion_pride.pdf')\n",
    "# \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "tn, fp, fn, tp = confusion_m.ravel()\n",
    "\n",
    "# Accuracy\n",
    "accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "\n",
    "# Sensitivity (Recall)\n",
    "sensitivity = tp / (tp+fn)\n",
    "\n",
    "# Specificity\n",
    "specificity = tn / (tn + fp)\n",
    "\n",
    "# PPV (Precision) from sklearn\n",
    "ppv = tp / (tp+fp)\n",
    "\n",
    "# NPV \n",
    "npv = tn / (tn + fn)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.3f}\")\n",
    "print(f\"Sensitivity (Recall): {sensitivity:.3f}\")\n",
    "print(f\"Specificity : {specificity:.3f}\")\n",
    "print(f\"PPV (Precision): {ppv:.3f}\")\n",
    "print(f\"NPV: {npv:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UMAP\n",
    "from umap import UMAP\n",
    "\n",
    "# processed and imputed\n",
    "features = topfeatures_identifiers\n",
    "umap_2d = UMAP(n_components=2, init='random', random_state=0)\n",
    "\n",
    "umap_df = pd.DataFrame(umap_2d.fit_transform(prepared_data[topfeatures_identifiers]))\n",
    "fig = px.scatter(umap_df, x=0, y=1,\n",
    "                 color=prepared_data.reset_index()['_diagnosis'],\n",
    "                 template = 'simple_white', height = 400, width = 900, facet_col=prepared_data.reset_index()['_study'])\n",
    "fig.show()\n",
    "#    fig.write_image('../submission/csf_UMAP_VM_festures_{}.pdf'.format(col))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML: LNB vs. control\n",
    "\n",
    "The strategy is the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepared_data = ml_data.copy()\n",
    "prepared_data=zscore(prepared_data, axis = 1)\n",
    "train_data = prepared_data.loc[(prepared_data.index.get_level_values('_study')=='csf_development')&(prepared_data.index.get_level_values('_diagnosis').isin(['LNB','control']))]\n",
    "\n",
    "no_cv = 5\n",
    "X = train_data.values # all the protein columns\n",
    "sample_lst = train_data.index.get_level_values('_sample').values\n",
    "y = pd.Categorical(train_data.index.get_level_values('_diagnosis')).codes # control is 0, LNB is 1 or LNB 0 and VM 1\n",
    "#X_train, X_test, y_train, y_test, samples_train, samples_test = train_test_split(X, y, train_data['sample'].tolist(), test_size=0.2, random_state=7, stratify=y)\n",
    "X_train = X.copy()\n",
    "y_train = y.copy()\n",
    "sum(y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data = prepared_data.loc[(prepared_data.index.get_level_values('_study')=='csf_validation')&(prepared_data.index.get_level_values('_diagnosis').isin(['LNB','control']))]\n",
    "X_val = val_data.values\n",
    "y_val = pd.Categorical(val_data.index.get_level_values('_diagnosis')).codes\n",
    "sum(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import compress\n",
    "protein_cols = gene_mapping_df.loc[combined_lst]['Gene Names'].tolist()\n",
    "# ML alg. selection\n",
    "optimal_models = []\n",
    "optimal_no_features = []\n",
    "optimal_features = []\n",
    "start_time = time.time()\n",
    "\n",
    "dfs = []\n",
    "metrics = []\n",
    "to_plot = []   \n",
    "fitted_models = []\n",
    "    \n",
    "for name, grid, estimator in models:\n",
    "    print(name)\n",
    "    # grid search\n",
    "    selector = RandomizedSearchCV(estimator=estimator, param_distributions=grid, cv=StratifiedKFold(no_cv), scoring = 'f1_weighted', n_jobs = -1)\n",
    "    clf = selector.fit(X_train,y_train)\n",
    "    optimal_model = clf.best_estimator_\n",
    "    optimal_models.append(optimal_model)\n",
    "    # feature reduction\n",
    "    if name in ['Naive Bayes','NNet', 'GP', 'QDA', 'KNN']: # skip rfecv for models not supported by importance\n",
    "        optimal_no_features.append(len(protein_cols))\n",
    "        optimal_features.append(protein_cols)\n",
    "        print('skipping feature importance')\n",
    "        X_reduced = X_train\n",
    "        X_val_reduced = X_val\n",
    "    else:\n",
    "        selector = RFECV(optimal_model, cv=5, scoring='f1_weighted')\n",
    "        selector = selector.fit(X_train, y_train)\n",
    "        optimal_no_features.append(sum(selector.support_))\n",
    "        optimal_features.append(list(compress(protein_cols, selector.support_)))\n",
    "        print('optimal no features = {}'.format(sum(selector.support_)))\n",
    "        X_reduced = X_train[:,selector.support_]\n",
    "        X_val_reduced = X_val[:, selector.support_]\n",
    "    \n",
    "    # compute cv results\n",
    "    print('computing predictions for: {}'.format(name))\n",
    "    classifier = optimal_model\n",
    "    mccs = []\n",
    "    aucs = []\n",
    "    fits = []\n",
    "    mcc_val = []\n",
    "    auc_val = []\n",
    "                            \n",
    "    # calculate performance during training\n",
    "    cv = StratifiedKFold(n_splits=no_cv)\n",
    "    for i, (train, test) in enumerate(cv.split(X_reduced, y_train)):\n",
    "        print('cv: {}'.format(i+1))\n",
    "        classifier.fit(X_reduced[train], y_train[train])\n",
    "        \n",
    "        fits.append(classifier)\n",
    "        # predict for \"test\" set\n",
    "        y_pred = classifier.predict(X_reduced[test])\n",
    "        y_prob = classifier.predict_proba(X_reduced[test])[:,1]\n",
    "        result = pd.DataFrame([y_train[test], y_pred, y_prob]).T\n",
    "        result.columns = ['observed','predicted','probability']\n",
    "        result['cv']=i+1\n",
    "        result['model']=name\n",
    "        result['sample']=sample_lst[test]\n",
    "        result['run']='test'\n",
    "        dfs.append(result)\n",
    "        mccs.append(round(matthews_corrcoef(result.observed, result.predicted),2))\n",
    "        aucs.append(round(roc_auc_score(result.observed,result.probability),2))\n",
    "        # predict for validation sets\n",
    "        y_pred = classifier.predict(X_val_reduced)\n",
    "        y_prob = classifier.predict_proba(X_val_reduced)[:,1]\n",
    "        result = pd.DataFrame([y_val, y_pred, y_prob]).T\n",
    "        result.columns = ['observed','predicted','probability']\n",
    "        result['cv']=i+1\n",
    "        result['model']=name\n",
    "        result['sample']=val_data.index.get_level_values('_sample').tolist()\n",
    "        result['run']='val'\n",
    "        dfs.append(result)  \n",
    "        mcc_val.append(round(matthews_corrcoef(result.observed, result.predicted),2))\n",
    "        auc_val.append(round(roc_auc_score(result.observed,result.probability),2))      \n",
    "        \n",
    "    fitted_models.append(fits)\n",
    "    #test\n",
    "    metrics_results = pd.DataFrame(mccs)\n",
    "    metrics_results.columns = ['MCC']\n",
    "    metrics_results['AUROC']=aucs\n",
    "    metrics_results['cv']=list(range(1,1+no_cv))\n",
    "    metrics_results['model']=name\n",
    "    metrics_results['run']='test'\n",
    "    metrics.append(metrics_results)\n",
    "\n",
    "    #val\n",
    "    metrics_results = pd.DataFrame(mccs)\n",
    "    metrics_results.columns = ['MCC']\n",
    "    metrics_results['AUROC']=aucs\n",
    "    metrics_results['cv']=list(range(1,1+no_cv))\n",
    "    metrics_results['model']=name\n",
    "    metrics_results['run']='val'\n",
    "    metrics.append(metrics_results)\n",
    "    \n",
    "    mcc_mean = np.mean(mccs)\n",
    "    mcc_std = np.std(mccs)\n",
    "    auroc_mean = np.mean(aucs)\n",
    "    auroc_std = np.std(aucs)\n",
    "    to_plot.append([name, mcc_mean, mcc_std, auroc_mean, auroc_std])\n",
    "    print('MCC: {}, AUROC: {}'.format(mcc_mean, auroc_mean))\n",
    "    \n",
    "    print('\\n{} model tested --- {} minutes ---\\n'.format(name,round(((time.time() - start_time)/60),3)))\n",
    "\n",
    "\n",
    "result_df = pd.concat(dfs)\n",
    "metrics_df = pd.concat(metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_df = pd.DataFrame(to_plot)\n",
    "plot_df.columns=['model','mcc_mean','mcc_std','auroc_mean','auroc_std']\n",
    "plot_df = round(plot_df,2)\n",
    "plot_df['no_features']=[len(x) for x in optimal_features]\n",
    "plot_df['features']=optimal_features\n",
    "plot_df = plot_df.sort_values('auroc_mean').reset_index(drop = True)\n",
    "plot_df['model_n_features']= ['{} (no. proteins = {})'.format(plot_df.loc[i,'model'],plot_df.loc[i,'no_features']) for i in plot_df.index.tolist()]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly.subplots import make_subplots\n",
    "butterfly_plot(plot_df, var1 = 'mcc_mean', var2 = 'auroc_mean', var3 = 'no_features', error1 = 'mcc_std', error2 = 'auroc_std', name = 's5', savesvg=True, group = 'model_n_features')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector = RFECV(optimal_models[5], cv=5, scoring='f1_weighted')\n",
    "selector = selector.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Optimal number of features: {selector.n_features_}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"SVC\"\n",
    "error_max = selector.cv_results_['mean_test_score']+selector.cv_results_['std_test_score']\n",
    "error_min = selector.cv_results_['mean_test_score']-selector.cv_results_['std_test_score']\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=np.concatenate([list(range(1,len(protein_cols)+1)),list(range(1,len(protein_cols)+1))[::-1]]),\n",
    "    y=np.concatenate([error_max, error_min[::-1]]),\n",
    "    fill='toself', opacity=0.5, name = '1 std.'))\n",
    "fig.add_trace(go.Scatter(x=list(range(1,len(protein_cols)+1)), y = selector.cv_results_['mean_test_score'],\n",
    "                         name='Mean test score',line = dict(color='firebrick', width=1)))\n",
    "fig.add_trace(go.Scatter(x = [selector.n_features_,selector.n_features_], name = 'Optimal number of features: {}'.format(selector.n_features_), \n",
    "                         y=[0,1], \n",
    "                         mode='lines', \n",
    "                         line=dict(color='green', width=2, dash='dash')))\n",
    "\n",
    "fig.update_layout(template='simple_white',\n",
    "    autosize=False,\n",
    "    width=800,\n",
    "    height=500,\n",
    "    title=\"RFECV for {} classifier\".format(name),\n",
    "    xaxis_title=\"Number of features selected\",\n",
    "    yaxis_title=\"F1-weighted\",\n",
    "    legend_title=\"\",\n",
    "    #legend=dict(y=1.1, orientation='h'),\n",
    "    legend_traceorder=\"reversed\",\n",
    "    xaxis_range=[0,46],\n",
    "    yaxis_range = [0,max(error_max)])\n",
    "\n",
    "#fig.write_image('../submission/RFECV_{}.pdf'.format(name))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topfeatures_identifiers = list(compress(protein_cols, selector.support_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = zscore(X_train[:, selector.support_], axis = 1)\n",
    "X_val = zscore(X_val[:, selector.support_], axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "# compute cv results\n",
    "print('computing predictions for: {}'.format(optimal_models[5]))\n",
    "classifier = optimal_models[5]\n",
    "name = 'SVC'\n",
    "mccs = []\n",
    "aucs = []\n",
    "fits = []\n",
    "mcc_val = []\n",
    "auc_val = []\n",
    "dfs = []\n",
    "                        \n",
    "# calculate performance during training \n",
    "cv = StratifiedKFold(n_splits=no_cv)\n",
    "for i, (train, test) in enumerate(cv.split(X_train, y_train)):\n",
    "    print('cv: {}'.format(i+1))\n",
    "    classifier.fit(X_train[train], y_train[train])\n",
    "    #calibrated = CalibratedClassifierCV(classifier, method='sigmoid', cv=5)\n",
    "    #calibrated.fit(X_train[train], y_train[train])\n",
    "    #classifier = calibrated\n",
    "    fits.append(classifier)\n",
    "    # predict for \"test\" set\n",
    "    y_pred = classifier.predict(X_train[test])\n",
    "    y_prob = classifier.predict_proba(X_train[test])[:,1]\n",
    "    result = pd.DataFrame([y_train[test], y_pred, y_prob]).T\n",
    "    result.columns = ['observed','predicted','probability']\n",
    "    result['cv']=i+1\n",
    "    result['model']=name\n",
    "    result['sample']=sample_lst[test]\n",
    "    result['run']='test'\n",
    "    dfs.append(result)\n",
    "    mccs.append(round(matthews_corrcoef(result.observed, result.predicted),2))\n",
    "    aucs.append(round(roc_auc_score(result.observed,result.probability),2))\n",
    "    # predict for validation sets\n",
    "    y_pred = classifier.predict(X_val)\n",
    "    y_prob = classifier.predict_proba(X_val)[:,1]\n",
    "    result = pd.DataFrame([y_val, y_pred, y_prob]).T\n",
    "    result.columns = ['observed','predicted','probability']\n",
    "    result['cv']=i+1\n",
    "    result['model']=name\n",
    "    result['sample']=val_data.index.get_level_values('_sample').tolist()\n",
    "    result['run']='val'\n",
    "    dfs.append(result)  \n",
    "    mcc_val.append(round(matthews_corrcoef(result.observed, result.predicted),2))\n",
    "    auc_val.append(round(roc_auc_score(result.observed,result.probability),2))      \n",
    "    \n",
    "mcc_mean = np.mean(mccs)\n",
    "mcc_std = np.std(mccs)\n",
    "auroc_mean = np.mean(aucs)\n",
    "auroc_std = np.std(aucs)\n",
    "to_plot.append([name, mcc_mean, mcc_std, auroc_mean, auroc_std])\n",
    "print('Test: MCC: {}, AUROC: {}'.format(mcc_mean, auroc_mean))\n",
    "print('Validation: MCC: {}, AUROC: {}'.format(np.mean(mcc_val), np.mean(auc_val)))\n",
    "\n",
    "result = pd.concat(dfs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"SVC\"\n",
    "\n",
    "fig = go.Figure()\n",
    "# ROC for TEST\n",
    "test_data = roc_data(result, model = name, run = \"test\")\n",
    "# output_order = test_data[0], test_data[1], test_data[2], test_data[3], test_data[4], auc_sd, ci_upper, ci_lower, mcc_mean, mcc_std\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=np.concatenate([test_data[0],test_data[0][::-1]]),\n",
    "    y=np.concatenate([test_data[2], test_data[3][::-1]]),\n",
    "    fill='toself', opacity=0.5, name = 'std.'))\n",
    "print('TEST auc = {} (std. = {}, ci = {}-{}), mcc = {} (std. = {}))'.format(test_data[4], test_data[5], test_data[6], test_data[7], test_data[8], test_data[9]))\n",
    "fig.add_trace(go.Scatter(x = test_data[0], name = 'TEST auc = {} (std. = {}, ci = {}-{}), mcc = {} (std. = {}))'.format(test_data[4], test_data[5], test_data[6], test_data[7], test_data[8], test_data[9]), \n",
    "                         y=test_data[1], \n",
    "                         mode='lines', \n",
    "                         line=dict(color='grey', width=2, dash='dash')))\n",
    "test_data = roc_data(result, model = name, run = \"val\")\n",
    "print('VAL auc = {} (std. = {}, ci = {}-{}), mcc = {} (std. = {}))'.format(test_data[4], test_data[5], test_data[6], test_data[7], test_data[8], test_data[9]))\n",
    "fig.add_trace(go.Scatter(x = test_data[0], name = 'VAL auc = {} (std. = {}, ci = {}-{}), mcc = {} (std. = {}))'.format(test_data[4], test_data[5], test_data[6], test_data[7], test_data[8], test_data[9]), \n",
    "                         y=test_data[1], \n",
    "                         mode='lines', \n",
    "                         line=dict(color='green', width=2)))\n",
    "\n",
    "fig.update_layout(template='simple_white',\n",
    "    autosize=False,\n",
    "    width=1000,\n",
    "    height=500,\n",
    "    title=\"AUROC for SVC classifier\",\n",
    "    xaxis_title=\"TPR\",\n",
    "    yaxis_title=\"FPR\",\n",
    "    legend_title=\"\",\n",
    "    legend_traceorder=\"reversed\",\n",
    "    xaxis_range=[0,1],\n",
    "    yaxis_range = [0,1])\n",
    "fig.write_image('../submission/f2c.svg', width = 950)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shaply feature importance\n",
    "import shap\n",
    "\n",
    "X_train_df = pd.DataFrame(X_train, columns = topfeatures_identifiers)\n",
    "#X_test_df = pd.DataFrame(X_val, columns = topfeatures_identifiers)\n",
    "\n",
    "# Generate the Tree SHAP estimator of Shapley values that corresponds to the LDA we built\n",
    "explainer = shap.LinearExplainer(optimal_models[5], X_train_df)\n",
    "# Compute the estimated Shapley values for the test sample's observations\n",
    "shap_values = explainer.shap_values(X_train_df)\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "shap.summary_plot(shap_values, X_train_df,max_display=17,show=False)\n",
    "\n",
    "plt.savefig('../submission/f2d.svg', format = 'svg') #.png,.pdf will also support here\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = result[(result.model==name)&(result.run=='val')].observed\n",
    "y_pred = result[(result.model==name)&(result.run=='val')].predicted\n",
    "confusion_m = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(3,3))\n",
    "s = sns.heatmap(confusion_m, annot = True, cbar = False, fmt = \"d\", linewidths = .5, cmap = \"Blues\",\n",
    "           xticklabels=['LNB','control'], yticklabels=['LNB','control'])\n",
    "s.set(xlabel=\"Predicted class\", ylabel='Actual class')\n",
    "s.set(title = \"Confusion Matrix\")\n",
    "fig.tight_layout()\n",
    "#fig.savefig('/Users/tzx804/projects/collaborations/nicolai/misc/sepsis/output/test_confusion_pride.pdf')\n",
    "# \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "tn, fp, fn, tp = confusion_m.ravel()\n",
    "\n",
    "# Accuracy\n",
    "accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "\n",
    "# Sensitivity (Recall)\n",
    "sensitivity = tp / (tp+fn)\n",
    "\n",
    "# Specificity\n",
    "specificity = tn / (tn + fp)\n",
    "\n",
    "# PPV (Precision) from sklearn\n",
    "ppv = tp / (tp+fp)\n",
    "\n",
    "# NPV \n",
    "npv = tn / (tn + fn)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.3f}\")\n",
    "print(f\"Sensitivity (Recall): {sensitivity:.3f}\")\n",
    "print(f\"Specificity : {specificity:.3f}\")\n",
    "print(f\"PPV (Precision): {ppv:.3f}\")\n",
    "print(f\"NPV: {npv:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UMAP\n",
    "from umap import UMAP\n",
    "\n",
    "# processed and imputed\n",
    "features = topfeatures_identifiers\n",
    "umap_2d = UMAP(n_components=2, init='random', random_state=0)\n",
    "\n",
    "umap_df = pd.DataFrame(umap_2d.fit_transform(prepared_data[topfeatures_identifiers]))\n",
    "fig = px.scatter(umap_df, x=0, y=1,\n",
    "                 color=prepared_data.reset_index()['_diagnosis'],\n",
    "                 template = 'simple_white', height = 400, width = 900, facet_col=prepared_data.reset_index()['_study'])\n",
    "fig.show()\n",
    "#    fig.write_image('../submission/csf_UMAP_VM_festures_{}.pdf'.format(col))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BBH (alphastats-project)",
   "language": "python",
   "name": "alphastats-project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
